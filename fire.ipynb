{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6d53231",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cvzone\n",
    "import cv2\n",
    "import math\n",
    "from tkinter import Tk\n",
    "from tkinter.filedialog import askopenfilename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34482624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 4 fires, 328.0ms\n",
      "Detected: fire, Confidence: 82%\n",
      "Speed: 15.2ms preprocess, 328.0ms inference, 16.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 fires, 195.6ms\n",
      "Speed: 5.0ms preprocess, 195.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No Fire Detected\n",
      "\n",
      "0: 480x640 1 fire, 176.4ms\n",
      "Speed: 3.0ms preprocess, 176.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No Fire Detected\n",
      "\n",
      "0: 480x640 2 fires, 180.8ms\n",
      "Speed: 1.9ms preprocess, 180.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No Fire Detected\n",
      "\n",
      "0: 480x640 2 fires, 177.6ms\n",
      "Speed: 2.0ms preprocess, 177.6ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No Fire Detected\n",
      "\n",
      "0: 480x640 2 fires, 178.5ms\n",
      "Speed: 2.0ms preprocess, 178.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No Fire Detected\n",
      "\n",
      "0: 480x640 2 fires, 175.8ms\n",
      "Speed: 1.0ms preprocess, 175.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No Fire Detected\n",
      "\n",
      "0: 480x640 1 fire, 174.4ms\n",
      "Speed: 2.0ms preprocess, 174.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No Fire Detected\n",
      "\n",
      "0: 480x640 1 fire, 173.8ms\n",
      "Speed: 2.1ms preprocess, 173.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No Fire Detected\n",
      "\n",
      "0: 480x640 2 fires, 172.1ms\n",
      "Speed: 2.0ms preprocess, 172.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No Fire Detected\n",
      "\n",
      "0: 480x640 1 fire, 174.2ms\n",
      "Speed: 3.0ms preprocess, 174.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No Fire Detected\n",
      "\n",
      "0: 480x640 1 fire, 174.3ms\n",
      "Speed: 2.0ms preprocess, 174.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No Fire Detected\n",
      "\n",
      "0: 480x640 1 fire, 175.4ms\n",
      "Speed: 2.0ms preprocess, 175.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No Fire Detected\n",
      "\n",
      "0: 480x640 1 fire, 175.6ms\n",
      "Speed: 2.0ms preprocess, 175.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No Fire Detected\n",
      "\n",
      "0: 480x640 1 fire, 170.9ms\n",
      "Speed: 2.0ms preprocess, 170.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No Fire Detected\n",
      "\n",
      "0: 480x640 1 fire, 173.5ms\n",
      "Speed: 2.0ms preprocess, 173.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No Fire Detected\n",
      "\n",
      "0: 480x640 (no detections), 194.4ms\n",
      "Speed: 3.1ms preprocess, 194.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No Fire Detected\n",
      "\n",
      "0: 480x640 1 fire, 181.0ms\n",
      "Speed: 2.0ms preprocess, 181.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No Fire Detected\n",
      "\n",
      "0: 480x640 1 fire, 175.0ms\n",
      "Speed: 1.0ms preprocess, 175.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No Fire Detected\n",
      "\n",
      "0: 480x640 2 fires, 183.2ms\n",
      "Speed: 2.0ms preprocess, 183.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No Fire Detected\n",
      "\n",
      "0: 480x640 2 fires, 176.1ms\n",
      "Speed: 1.0ms preprocess, 176.1ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No Fire Detected\n",
      "\n",
      "0: 480x640 1 fire, 176.1ms\n",
      "Speed: 2.0ms preprocess, 176.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No Fire Detected\n",
      "\n",
      "0: 480x640 1 fire, 177.3ms\n",
      "Speed: 1.0ms preprocess, 177.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No Fire Detected\n",
      "\n",
      "0: 480x640 1 fire, 203.7ms\n",
      "Speed: 2.0ms preprocess, 203.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No Fire Detected\n",
      "\n",
      "0: 480x640 2 fires, 193.4ms\n",
      "Speed: 2.0ms preprocess, 193.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "No Fire Detected\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Tk().withdraw()  # We don't want a full GUI, so keep the root window from appearing\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "model = YOLO('best.pt')\n",
    "\n",
    "# Class name for fire\n",
    "classnames = ['fire']\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break  # Break the loop if the video ends or no frame is captured\n",
    "\n",
    "    frame = cv2.resize(frame, (640, 480))  # Resize the frame for faster processing\n",
    "    results = model(frame, stream=True)  # Run the model inference\n",
    "\n",
    "    fire_detected = False  \n",
    "    for result in results:\n",
    "        boxes = result.boxes\n",
    "        for box in boxes:\n",
    "            confidence = box.conf[0]\n",
    "            confidence = math.ceil(confidence * 100)  # Convert to percentage\n",
    "            Class = int(box.cls[0])\n",
    "\n",
    "            # If the confidence is higher than a certain threshold (e.g., 60)\n",
    "            if confidence > 60:\n",
    "                fire_detected = True  # Set flag to True if fire is detected\n",
    "                x1, y1, x2, y2 = box.xyxy[0]\n",
    "                x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "\n",
    "                # Draw a rectangle around the detected fire region\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 3)\n",
    "\n",
    "                # Display the class name and confidence percentage\n",
    "                cvzone.putTextRect(frame, f'{classnames[Class]} {confidence}%', [x1 + 5, y1 - 10],\n",
    "                                   scale=1.5, thickness=2, offset=5, colorR=(255, 0, 0))\n",
    "\n",
    "                # Print the detection results in the terminal for validation\n",
    "                print(f'Detected: {classnames[Class]}, Confidence: {confidence}%')\n",
    "\n",
    "    # If no fire was detected in the current frame\n",
    "    if not fire_detected:\n",
    "        print('No Fire Detected')\n",
    "\n",
    "    # Display the video frame\n",
    "    cv2.imshow('Fire Detection', frame)\n",
    "\n",
    "    # Add a delay of 1ms to allow the frame to be displayed properly\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break  # Exit if 'q' is pressed\n",
    "\n",
    "# Release the video capture object and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
